# Only import modules from standard library here. Everything else
# should be imported from within function definition blocks (not
# starting with task_). This way, task_sync() gets a chance to update
# dependencies transparently even for this module
import copy
import json
from pathlib import Path
from urllib.request import urlopen


DOIT_CONFIG = {
    'action_string_formatting': 'new',
    'default_tasks': [
        'jsonschema',
        'pid_schema',
        'vocabularies',
        'python',
        # 'convert',
    ],
}
HERE = Path(__file__).parent
SRC_DOCS_DIR = HERE / 'src' / 'docs'
DOCS_DIR = HERE / 'docs'
SCHEMA_OVERVIEW = DOCS_DIR / 'schema_overview.md'
ER_DIAGRAM = HERE / 'avefi_er_diagram.md'
SITE_DIR = HERE / 'site'
UTILS_DIR = HERE / 'utils'
WORKING_DIR = HERE / 'KIP_DTR'
SCHEMA_NAME = 'avefi_schema'
SRC_SCHEMA_DIR = HERE / 'src' / SCHEMA_NAME
SRC_MODEL = SRC_SCHEMA_DIR / 'model.yaml'
SRC_SCHEMA_DEPENDENCIES = [
    SRC_MODEL,
    SRC_SCHEMA_DIR / 'vocab.yaml',
]
PROJECT_DIR = HERE / 'project'
JSON_SCHEMA = PROJECT_DIR / 'jsonschema' / SCHEMA_NAME \
    / f"{SRC_MODEL.stem}.schema.json"
EPIC_SCHEMA_DIR = PROJECT_DIR / 'jsonschema' / 'epic'
EPIC_VOCAB_DIR = EPIC_SCHEMA_DIR / 'vocabularies'
EPIC_VOCAB_BASE_URL = \
    f"https://raw.githubusercontent.com/AV-EFI/av-efi-schema/main/"\
    f"{'/'.join(EPIC_VOCAB_DIR.relative_to(HERE).parts)}/"
PID_SCHEMAS = [
    EPIC_SCHEMA_DIR / f"{SRC_MODEL.stem}_{subschema}.schema.json"
    for subschema in ('workvariant', 'manifestation', 'item')
]


SCHEMA_PIDS = {
    'work': '21.T11148/31b848e871121c47d064',
    'manifestation': '21.T11148/ef6836b80e4d64e574e3',
    'item': '21.T11148/b0047df54c686b9df82a',
}
#TYPEAPI = 'http://typeapi.pidconsortium.net/dtype/schema/JSON/'
#REQUEST_PARAMS = '/?cached=true'
TYPEAPI = 'http://typeapi.lab.pidconsortium.net/v1/types/schema/'
REQUEST_PARAMS = '?refresh=true'


def task_vocabularies():
    """Extract enum lists for typeapi from autogenerated json schema."""
    return {
        'actions': [
            'mkdir -p {targets}',
            generate_json_enum_files,
        ],
        'file_dep': [JSON_SCHEMA],
        'targets': [EPIC_VOCAB_DIR],
        'clean': ['rm -rf {targets}'],
    }


def generate_json_enum_files(dependencies, targets):
    """Generate vocabularies for use in the data type registry

    Results can be retrieved from
    https://raw.githubusercontent.com/AV-EFI/av-efi-schema/main/project/jsonschema/epic/vocabularies/

    """
    with open(dependencies[0], 'r') as f:
        schema = json.load(f)
    for key, value in schema['$defs'].items():
        if isinstance(value, dict):
            if 'enum' in value.keys():
                output = {'$id': key}
                output.update(value)
                output_path = EPIC_VOCAB_DIR / f"{key}.json"
                with output_path.open('w') as f:
                    json.dump(output, f, indent=2)
                    f.write('\n')


def expand_and_split_json_schema(dependencies, targets):
    """Generate separate schemas for work, manifestation and item."""
    import jsonref

    schema_path = Path(dependencies[0])
    with schema_path.open('r') as f:
        schema = jsonref.load(f, jsonschema=True)
    jsonref._walk_refs(schema, _expand_refs_except_enums, replace=True)
    for i in range(len(schema['properties']['has_record']['anyOf'])):
        name = schema['properties']['has_record']['anyOf'][i]['title']
        output = copy.deepcopy(schema)
        output['$id'] = f"{schema['$id']}-{name.lower()}"
        output['title'] += f" for {name}"
        output['description'] = \
            f"Auto-generated from {schema['$id']} for {name} PIDs"
        output['properties']['has_record'] = \
            output['properties']['has_record']['anyOf'][i]
        del output['$defs']
        output_path = EPIC_SCHEMA_DIR / \
            f"{SRC_MODEL.stem}_{name.lower()}.schema.json"
        assert output_path in PID_SCHEMAS, \
            f"Abort generating {output_path} because it is not listed in" \
            f" {PID_SCHEMAS}"
        with output_path.open('w') as f:
            jsonref.dump(output, f, indent=4)


def _expand_refs_except_enums(obj):
    """Expand internal references and make category a const."""
    if obj.__reference__['$ref'].endswith('Enum'):
        # Replace by external $ref to enum in Github repo
        result = obj.__reference__
        enum_name = result['$ref'].split('/')[-1]
        result['$ref'] = f"{EPIC_VOCAB_BASE_URL}{enum_name}.json"
    else:
        # Replace by referenced object and do some minor transformations
        result = obj.__subject__
        properties = result.get('properties')
        if properties:
            category = properties.get('category')
            if category and 'enum' in category:
                category['const'] = category['enum'][0]
                del category['enum']
                del category['type']
    return result


def task_pid_schema():
    """Generate derived schemas for WorkVariant, Manifestation, Item."""
    return {
        'actions': [expand_and_split_json_schema],
        'file_dep': [JSON_SCHEMA],
        'task_dep': ['sync_dependencies'],
        'targets': PID_SCHEMAS,
    }


def task_jsonschema():
    """Generate derived JSON Schema."""
    return {
        'actions': [
            f"gen-json-schema --closed --title-from title {SRC_MODEL}"
            f" > {{targets}}",
        ],
        'task_dep': ['sync_dependencies'],
        'file_dep': SRC_SCHEMA_DEPENDENCIES,
        'targets': [JSON_SCHEMA],
    }


def task_python():
    """Generate python bindings.

    Even though the designator slot "category" is declared required in
    the schema, let it be treated as optional by the generated
    bindings. This is convenient when constructors can determine that
    value automatically on instantiation.

    """
    def generate_bindings(module_name, class_name, source, target, **kwargs):
        import importlib
        gen_module = importlib.import_module(
            f"linkml.generators.{module_name}")
        gen = getattr(gen_module, class_name)(source, **kwargs)
        # Make category slot optional and rely on constructors to fill
        # it correctly
        if gen.schema.slots['category'].designates_type:
            gen.schema.slots['category'].required = False
        with open(target, 'w') as f:
            f.write(gen.serialize())

    python_model = PROJECT_DIR / 'python' / SCHEMA_NAME \
        / f"{SRC_MODEL.stem}.py"
    for module, cls, target, kwargs in [
            ('pythongen', 'PythonGenerator', python_model, {}),
            ('pydanticgen', 'PydanticGenerator',
             python_model.with_stem(f"{python_model.stem}_pydantic_v2"),
             {'pydantic_version': 2}),
    ]:
        yield {
            'name': module,
            'actions': [
                (generate_bindings, [module, cls, SRC_MODEL, target],
                 kwargs),
            ],
            'task_dep': ['sync_dependencies'],
            'file_dep': SRC_SCHEMA_DEPENDENCIES,
            'targets': [target],
        }


def task_typescript():
    """Generate typescript derivatives."""
    typescript_path = PROJECT_DIR / 'typescript' / f"{SCHEMA_NAME}.ts"
    for cmd, target in [
            ('gen-typescript', typescript_path),
            ('gen-typescript --gen-type-utils', typescript_path.with_stem(
                f"{typescript_path.stem}_type_utils")),
    ]:
        yield {
            'name': cmd,
            'actions': [
                f"{cmd} {SRC_MODEL} > {{targets}}",
            ],
            'task_dep': ['sync_dependencies'],
            'file_dep': SRC_SCHEMA_DEPENDENCIES,
            'targets': [target],
        }


def task_docs():
    """Build documentation from LinkML schema."""
    return {
        'actions': [
            gen_doc,
        ],
        'task_dep': [
            'sync_dependencies',
            'copy_src_docs',
            'diagram',
        ],
        'file_dep': SRC_SCHEMA_DEPENDENCIES,
        'targets': [SCHEMA_OVERVIEW],
        'clean': [f"rm -rf {DOCS_DIR}"],
    }


def gen_doc(dependencies, targets):
    """Essentially gen-doc tuned for less aggressive cut offs."""
    import re
    from linkml.generators import docgen

    # Be less aggressive about truncating long lines for tables
    def enshorten(input):
        """Custom filter to truncate long text intended to go in a table.

        Remove anything after a newline but do not cut off after full
        stops. This is required to preserve links.

        """
        if input is None:
            return ""
        match = re.search(r'^(.*?([.;?!] |\n|$))', input)
        input = match.group()
        return input
    docgen.enshorten = enshorten

    index_file = Path(targets[0])
    gen = docgen.DocGenerator(
        SRC_MODEL,
        directory=index_file.parent,
        hierarchical_class_view=False,
        index_name=index_file.stem,
        sort_by='rank',
    )
    print(gen.serialize())


def task_diagram():
    """Generate diagram from LinkML schema."""
    return {
        'actions': [
            "gen-erdiagram -c WorkVariant -c Manifestation"
            " -c Item --follow-references {dependencies} > {targets}",
        ],
        'task_dep': ['sync_dependencies'],
        'file_dep': [SRC_MODEL],
        'targets': [ER_DIAGRAM],
    }


def task_copy_src_docs():
    """Copy files over from src/docs."""
    dependencies = list(SRC_DOCS_DIR.glob('*.md'))
    targets = [DOCS_DIR / d.name for d in dependencies]
    return {
        'actions': [
            f"mkdir -p {DOCS_DIR}",
            f"cp -rf {{dependencies}} {DOCS_DIR}",
        ],
        'file_dep': dependencies,
        'targets': targets,
    }


def task_build_pages():
    """Build ."""
    return {
        'actions': [
            "mkdocs serve",
        ],
        'task_dep': ['sync_dependencies', 'docs']
    }


def task_build_site():
    """Build static site from provided documentation."""
    return {
        'actions': [
            'mkdocs build',
        ],
        'task_dep': ['docs'],
        'file_dep': [SCHEMA_OVERVIEW],
        'targets': [SITE_DIR / 'schema_overview' / 'index.html'],
        'clean': [f"rm -rf {SITE_DIR}"],
    }


def task_serve_site():
    """Serve documentation on localhost for testing."""
    return {
        'actions': [
            'mkdocs serve',
        ],
        'task_dep': ['build_site'],
    }


def task_deploy_site():
    """Deploy docs to GitHub Pages."""
    return {
        'actions': [
            'mkdocs gh-deploy',
        ],
        'task_dep': ['build_site'],
    }


def task_sync_dependencies():
    """Install dependencies according to pdm.lock (for developers)."""
    return {
        'actions': [
            'pdm sync',
        ],
        'file_dep': ['pdm.lock'],
    }


def task_update_linkml():
    """Update dependencies (linkml, etc.)."""
    return {
        'actions': [
            'pdm update -u',
        ],
        'uptodate': (True,),
    }


def task_fetch_efi_schemas():
    """Fetch EFI JSON Schemas from the Data Type Registry."""
    def fetch_efi_schema(task):
        efi_type = task.name.rpartition(':')[2]
        pid = SCHEMA_PIDS[efi_type]
        response = urlopen(f"{TYPEAPI}{pid}{REQUEST_PARAMS}")
        jsondata = json.loads(response.read())
        if 'error' in jsondata.get('status', '').lower():
            raise RuntimeError(
                f"Request to {response.url} yielded this response: {jsondata}")
        with open(task.targets[0], 'w+') as f:
            f.write(json.dumps(jsondata, indent=4, sort_keys=True))

    for efi_type in SCHEMA_PIDS.keys():
        yield {
            'name': efi_type,
            'actions': [
                fetch_efi_schema,
            ],
            'targets': [
                WORKING_DIR / f"schema_{efi_type}_DTR.json",
            ],
            'clean': True,
            'uptodate': (True,),
            'verbosity': 2}


def task_convert():
    """Convert EFI Schemas from JSON into reStructuredText."""
    invoke = {
        'json2csv': f"python {UTILS_DIR / 'efischema2csv.py'}",
        'csv2rst':  f"python {UTILS_DIR / 'third_party' / 'csv2rst.py'} -w 50",
    }
    for efi_type in SCHEMA_PIDS.keys():
        for src, dst in (('json', 'csv'), ('csv', 'rst')):
            yield {
                'name': f"{efi_type}_{src}2{dst}",
                'actions': [
                    ' '.join([
                        invoke[f"{src}2{dst}"],
                        '-i', '{dependencies}', '-o', '{targets}',
                    ]),
                ],
                'file_dep': [
                    WORKING_DIR / f"schema_{efi_type}_DTR.{src}",
                ],
                'targets': [
                    WORKING_DIR / f"schema_{efi_type}_DTR.{dst}",
                ],
                'verbosity': 2,
            }

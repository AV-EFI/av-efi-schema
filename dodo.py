import copy
import json
import jsonref
from pathlib import Path
from urllib.request import urlopen


DOIT_CONFIG = {
    'action_string_formatting': 'new',
    'default_tasks': [
        'jsonschema',
        'pid_schema',
        'vocabularies',
        'python',
        # 'convert',
    ],
}
HERE = Path(__file__).parent
SRC_DOCS_DIR = HERE / 'src' / 'docs'
DOCS_DIR = HERE / 'docs'
SCHEMA_OVERVIEW = DOCS_DIR / 'schema_overview.md'
ER_DIAGRAM = HERE / 'avefi_er_diagram.md'
UTILS_DIR = HERE / 'utils'
WORKING_DIR = HERE / 'KIP_DTR'
SCHEMA_NAME = 'avefi_schema'
SRC_SCHEMA_DIR = HERE / 'src' / SCHEMA_NAME
SRC_MODEL = SRC_SCHEMA_DIR / 'model.yaml'
SRC_SCHEMA_DEPENDENCIES = [
    SRC_MODEL,
    SRC_SCHEMA_DIR / 'vocab.yaml',
]
PROJECT_DIR = HERE / 'project'
JSON_SCHEMA = PROJECT_DIR / 'jsonschema' / SCHEMA_NAME \
    / f"{SRC_MODEL.stem}.schema.json"
EPIC_SCHEMA_DIR = PROJECT_DIR / 'jsonschema' / 'epic'
EPIC_VOCAB_DIR = EPIC_SCHEMA_DIR / 'vocabularies'
PID_SCHEMAS = [
    EPIC_SCHEMA_DIR / f"{SRC_MODEL.stem}_{subschema}.schema.json"
    for subschema in ('workvariant', 'manifestation', 'item')
]


SCHEMA_PIDS = {
    'work': '21.T11148/31b848e871121c47d064',
    'manifestation': '21.T11148/ef6836b80e4d64e574e3',
    'item': '21.T11148/b0047df54c686b9df82a',
}
#TYPEAPI = 'http://typeapi.pidconsortium.net/dtype/schema/JSON/'
#REQUEST_PARAMS = '/?cached=true'
TYPEAPI = 'http://typeapi.lab.pidconsortium.net/v1/types/schema/'
REQUEST_PARAMS = '?refresh=true'


def task_vocabularies():
    """Extract enum lists for typeapi from autogenerated json schema."""
    return {
        'actions': [
            'mkdir -p {targets}',
            generate_json_enum_files,
        ],
        'file_dep': [JSON_SCHEMA],
        'targets': [EPIC_VOCAB_DIR],
        'clean': ['rm -rf {targets}'],
    }


def generate_json_enum_files(dependencies, targets):
    """Generate vocabularies for use in the data type registry

    Results can be retrieved from
    https://raw.githubusercontent.com/AV-EFI/av-efi-schema/main/project/jsonschema/epic/vocabularies/

    """
    with open(dependencies[0], 'r') as f:
        schema = json.load(f)
    for key, value in schema['$defs'].items():
        if isinstance(value, dict):
            if 'enum' in value.keys():
                output = {'$id': key}
                output.update(value)
                output_path = EPIC_VOCAB_DIR / f"{key}.json"
                with output_path.open('w') as f:
                    json.dump(output, f, indent=2)
                    f.write('\n')


def expand_and_split_json_schema(dependencies, targets):
    """Generate separate schemas for work, manifestation and item."""
    schema_path = Path(dependencies[0])
    with schema_path.open('r') as f:
        schema = jsonref.load(f, proxies=False, jsonschema=True)
    for i in range(len(schema['properties']['has_record']['items']['anyOf'])):
        name = schema['properties']['has_record']['items']['anyOf'][i]['title']
        output = copy.deepcopy(schema)
        output['$id'] = f"{schema['$id']}-{name.lower()}"
        output['title'] += f" for {name}"
        output['description'] = \
            f"Auto-generated from {schema['$id']} for {name} PIDs"
        output['properties']['has_record'] = \
            output['properties']['has_record']['items']['anyOf'][i]
        del output['$defs']
        output_path = EPIC_SCHEMA_DIR / \
            f"{SRC_MODEL.stem}_{name.lower()}.schema.json"
        assert output_path in PID_SCHEMAS, \
            f"Abort generating {output_path} because it is not listed in" \
            f" {PID_SCHEMAS}"
        with output_path.open('w') as f:
            jsonref.dump(output, f, indent=4)


def task_pid_schema():
    """Generate derived schemas for WorkVariant, Manifestation, Item."""
    return {
        'actions': [expand_and_split_json_schema],
        'file_dep': [JSON_SCHEMA],
        'task_dep': ['sync_dependencies'],
        'targets': PID_SCHEMAS,
    }


def task_jsonschema():
    """Generate derived JSON Schema."""
    return {
        'actions': [
            f"gen-json-schema --closed --title-from title {SRC_MODEL}"
            f" > {{targets}}",
        ],
        'task_dep': ['sync_dependencies'],
        'file_dep': SRC_SCHEMA_DEPENDENCIES,
        'targets': [JSON_SCHEMA],
    }


def task_python():
    """Generate python bindings."""
    python_model = PROJECT_DIR / 'python' / SCHEMA_NAME \
        / f"{SRC_MODEL.stem}.py"
    for cmd, target in [
            ('gen-python', python_model),
            ('gen-pydantic --pydantic-version 2', python_model.with_stem(
                f"{python_model.stem}_pydantic_v2")),
            ('gen-pydantic --pydantic-version 1', python_model.with_stem(
                f"{python_model.stem}_pydantic_v1")),
    ]:
        yield {
            'name': cmd,
            'actions': [
                f"{cmd} {SRC_MODEL} > {{targets}}",
            ],
            'task_dep': ['sync_dependencies'],
            'file_dep': SRC_SCHEMA_DEPENDENCIES,
            'targets': [target],
        }


def task_typescript():
    """Generate typescript derivatives."""
    typescript_path = PROJECT_DIR / 'typescript' / f"{SCHEMA_NAME}.ts"
    for cmd, target in [
            ('gen-typescript', typescript_path),
            ('gen-typescript --gen-type-utils', typescript_path.with_stem(
                f"{typescript_path.stem}_type_utils")),
    ]:
        yield {
            'name': cmd,
            'actions': [
                f"{cmd} {SRC_MODEL} > {{targets}}",
            ],
            'task_dep': ['sync_dependencies'],
            'file_dep': SRC_SCHEMA_DEPENDENCIES,
            'targets': [target],
        }


def task_docs():
    """Build documentation from LinkML schema."""
    return {
        'actions': [
            gen_doc,
        ],
        'task_dep': [
            'sync_dependencies',
            'copy_src_docs',
            'diagram',
        ],
        'file_dep': SRC_SCHEMA_DEPENDENCIES,
        'targets': [SCHEMA_OVERVIEW],
    }


def gen_doc(dependencies, targets):
    """Essentially gen-doc tuned for less aggressive cut offs."""
    import re
    from linkml.generators import docgen

    # Be less aggressive about truncating long lines for tables
    def enshorten(input):
        """Custom filter to truncate long text intended to go in a table.

        Remove anything after a newline but do not cut off after full
        stops. This is required to preserve links.

        """
        if input is None:
            return ""
        match = re.search(r'^(.*?([.;?!] |\n|$))', input)
        input = match.group()
        return input
    docgen.enshorten = enshorten

    index_file = Path(targets[0])
    gen = docgen.DocGenerator(
        SRC_MODEL,
        directory=index_file.parent,
        hierarchical_class_view=False,
        index_name=index_file.stem,
        sort_by='rank',
    )
    print(gen.serialize())


def task_diagram():
    """Generate diagram from LinkML schema."""
    return {
        'actions': [
            "gen-erdiagram -c WorkVariant -c Manifestation"
            " -c Item {dependencies} > {targets}",
        ],
        'task_dep': ['sync_dependencies'],
        'file_dep': [SRC_MODEL],
        'targets': [ER_DIAGRAM],
    }


def task_copy_src_docs():
    """Copy files over from src/docs."""
    dependencies = list(SRC_DOCS_DIR.glob('*.md'))
    targets = [DOCS_DIR / d.name for d in dependencies]
    return {
        'actions': [
            f"mkdir -p {DOCS_DIR}",
            f"cp -rf {{dependencies}} {DOCS_DIR}",
        ],
        'file_dep': dependencies,
        'targets': targets,
    }


def task_sync_dependencies():
    """Install dependencies according to pdm.lock (for developers)."""
    return {
        'actions': [
            'pdm sync',
        ],
        'file_dep': ['pdm.lock'],
    }


def task_update_linkml():
    """Update dependencies (linkml, etc.)."""
    return {
        'actions': [
            'pdm update -u',
        ],
        'uptodate': (True,),
    }


def task_fetch_efi_schemas():
    """Fetch EFI JSON Schemas from the Data Type Registry."""
    def fetch_efi_schema(task):
        efi_type = task.name.rpartition(':')[2]
        pid = SCHEMA_PIDS[efi_type]
        response = urlopen(f"{TYPEAPI}{pid}{REQUEST_PARAMS}")
        jsondata = json.loads(response.read())
        if 'error' in jsondata.get('status', '').lower():
            raise RuntimeError(
                f"Request to {response.url} yielded this response: {jsondata}")
        with open(task.targets[0], 'w+') as f:
            f.write(json.dumps(jsondata, indent=4, sort_keys=True))

    for efi_type in SCHEMA_PIDS.keys():
        yield {
            'name': efi_type,
            'actions': [
                fetch_efi_schema,
            ],
            'targets': [
                WORKING_DIR / f"schema_{efi_type}_DTR.json",
            ],
            'clean': True,
            'uptodate': (True,),
            'verbosity': 2}


def task_convert():
    """Convert EFI Schemas from JSON into reStructuredText."""
    invoke = {
        'json2csv': f"python {UTILS_DIR / 'efischema2csv.py'}",
        'csv2rst':  f"python {UTILS_DIR / 'third_party' / 'csv2rst.py'} -w 50",
    }
    for efi_type in SCHEMA_PIDS.keys():
        for src, dst in (('json', 'csv'), ('csv', 'rst')):
            yield {
                'name': f"{efi_type}_{src}2{dst}",
                'actions': [
                    ' '.join([
                        invoke[f"{src}2{dst}"],
                        '-i', '{dependencies}', '-o', '{targets}',
                    ]),
                ],
                'file_dep': [
                    WORKING_DIR / f"schema_{efi_type}_DTR.{src}",
                ],
                'targets': [
                    WORKING_DIR / f"schema_{efi_type}_DTR.{dst}",
                ],
                'verbosity': 2,
            }
